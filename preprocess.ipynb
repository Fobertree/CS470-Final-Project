{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b2fcf89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d64fdabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(trainDF, testDF):\n",
    "    scaler = StandardScaler()\n",
    "    trainDF = scaler.fit_transform(trainDF)\n",
    "    testDF = scaler.fit_transform(testDF)\n",
    "    return trainDF, testDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7237e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_gridsearch(clf, pgrid, xTrain, yTrain, xTest, yTest):\n",
    "\n",
    "    cv = GridSearchCV(clf, param_grid=pgrid, cv=10)\n",
    "    cv.fit(xTrain, yTrain)\n",
    "\n",
    "    clf = cv.best_estimator_\n",
    "    best_params = cv.best_params_\n",
    "\n",
    "    yHat = cv.predict(xTest)\n",
    "    yHat_proba = cv.predict_proba(xTest)[:,1]\n",
    "\n",
    "    auc = roc_auc_score(yTest, yHat_proba)\n",
    "\n",
    "    auprc = average_precision_score(yTest, yHat_proba)\n",
    "\n",
    "    f1 = f1_score(yTest, yHat)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(yTest, yHat_proba)\n",
    "\n",
    "    return {'AUC': auc, 'AUPRC': auprc, 'F1': f1}, {'fpr': fpr, 'tpr': tpr}, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "103ff41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_randomsearch(clf, pgrid, xTrain, yTrain, xTest, yTest):\n",
    "    permutations = np.prod([len(v) for v in pgrid.values()])\n",
    "\n",
    "    cv = RandomizedSearchCV(clf, param_distributions=pgrid, n_iter=int(permutations*0.33), cv=10)\n",
    "    cv.fit(xTrain, yTrain)\n",
    "\n",
    "    clf = cv.best_estimator_\n",
    "    best_params = cv.best_params_\n",
    "\n",
    "    yHat = cv.predict(xTest)\n",
    "    yHat_proba = cv.predict_proba(xTest)[:,1]\n",
    "\n",
    "    auc = roc_auc_score(yTest, yHat_proba)\n",
    "\n",
    "    auprc = average_precision_score(yTest, yHat_proba)\n",
    "\n",
    "    f1 = f1_score(yTest, yHat)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(yTest, yHat_proba)\n",
    "\n",
    "    return {'AUC': auc, 'AUPRC': auprc, 'F1': f1}, {'fpr': fpr, 'tpr': tpr}, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "32972202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter_grid(mName):\n",
    "    if mName == 'LR (None)':\n",
    "        return {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'tol': [0.0001, 0.0004]}\n",
    "    elif mName == 'LR (L1)':\n",
    "        return {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'tol': [0.0001, 0.0004]}\n",
    "    elif mName == 'LR (L2)':\n",
    "        return {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'tol': [0.0001, 0.0004]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "896c807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_searchcv(clfName, clf, clfGrid,\n",
    "                  xTrain, yTrain, xTest, yTest,\n",
    "                  perfDict, rocDF, bestParamDict):\n",
    "    # evaluate grid search and add to perfDict\n",
    "    cls_perf, cls_roc, gs_p  = eval_gridsearch(clf, clfGrid, xTrain,\n",
    "                                               yTrain, xTest, yTest)\n",
    "    perfDict[clfName + \" (Grid)\"] = cls_perf\n",
    "    # add to ROC DF\n",
    "    rocRes = pd.DataFrame(cls_roc)\n",
    "    rocRes[\"model\"] = clfName\n",
    "    rocDF = pd.concat([rocDF, rocRes], ignore_index=True)\n",
    "    # evaluate random search and add to perfDict\n",
    "    clfr_perf, _, rs_p  = eval_randomsearch(clf, clfGrid, xTrain,\n",
    "                                            yTrain, xTest, yTest)\n",
    "    perfDict[clfName + \" (Random)\"] = clfr_perf\n",
    "    bestParamDict[clfName] = {\"Grid\": gs_p, \"Random\": rs_p}\n",
    "    return perfDict, rocDF, bestParamDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145fe63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df = pd.read_csv(\"Models/Data/data.csv\")\n",
    "\n",
    "    # make classes\n",
    "    X = df.drop(columns=['close', 'otc'])\n",
    "    y = pd.DataFrame(columns=['price'])\n",
    "    for i, row in df.iterrows():\n",
    "        if row['open'] - row['close'] > 0:\n",
    "            y.loc[i] = 1 # decrease\n",
    "        else:\n",
    "            y.loc[i] = 0 # increase\n",
    "\n",
    "    y = y.to_numpy().flatten()\n",
    "\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    xTrain, xTest = preprocess(xTrain, xTest)\n",
    "\n",
    "    perfDict = {}\n",
    "    rocDF = pd.DataFrame()\n",
    "    bestParamDict = {}\n",
    "\n",
    "    print(\"Tuning Unregularized Logistic Regression --------\")\n",
    "    # logistic regression (unregularized)\n",
    "    unregLrName = \"LR (None)\"\n",
    "    unregLrGrid = get_parameter_grid(unregLrName)\n",
    "    # fill in\n",
    "    lrClf = LogisticRegression()\n",
    "    perfDict, rocDF, bestParamDict = eval_searchcv(unregLrName, lrClf, unregLrGrid,\n",
    "                                                   xTrain, yTrain, xTest, yTest,\n",
    "                                                   perfDict, rocDF, bestParamDict)\n",
    "    # logistic regression (L1)\n",
    "    print(\"Tuning Logistic Regression (Lasso) --------\")\n",
    "    lassoLrName = \"LR (L1)\"\n",
    "    lassoLrGrid = get_parameter_grid(lassoLrName)\n",
    "    # fill in\n",
    "    lassoClf = LogisticRegression(penalty='l1', solver='liblinear', max_iter=300)\n",
    "    perfDict, rocDF, bestParamDict = eval_searchcv(lassoLrName, lassoClf, lassoLrGrid,\n",
    "                                                   xTrain, yTrain, xTest, yTest,\n",
    "                                                   perfDict, rocDF, bestParamDict)\n",
    "    # Logistic regression (L2)\n",
    "    print(\"Tuning Logistic Regression (Ridge) --------\")\n",
    "    ridgeLrName = \"LR (L2)\"\n",
    "    ridgeLrGrid = get_parameter_grid(ridgeLrName)\n",
    "    # fill in\n",
    "    ridgeClf = LogisticRegression(penalty='l2')\n",
    "    perfDict, rocDF, bestParamDict = eval_searchcv(ridgeLrName, ridgeClf, ridgeLrGrid,\n",
    "                                                   xTrain, yTrain, xTest, yTest,\n",
    "                                                   perfDict, rocDF, bestParamDict)\n",
    "\n",
    "    perfDF = pd.DataFrame.from_dict(perfDict, orient='index')\n",
    "    print(perfDF)\n",
    "    # save roc curves to data\n",
    "    rocDF.to_csv('out', index=False)\n",
    "    # store the best parameters\n",
    "    with open('best', 'w') as f:\n",
    "        json.dump(bestParamDict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "069f3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout(model, xFeat, y, testSize):\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(xFeat, y, test_size=testSize)\n",
    "    resultDict = eval_randomsearch(model, xTrain, yTrain, xTest, yTest)\n",
    "    return resultDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "97b95a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Unregularized Logistic Regression --------\n",
      "Tuning Logistic Regression (Lasso) --------\n",
      "Tuning Logistic Regression (Ridge) --------\n",
      "                         AUC     AUPRC        F1\n",
      "LR (None) (Grid)    0.897898  0.849056  0.816901\n",
      "LR (None) (Random)  0.900901  0.851657  0.828571\n",
      "LR (L1) (Grid)      0.891141  0.846480  0.828571\n",
      "LR (L1) (Random)    0.890390  0.858577  0.816901\n",
      "LR (L2) (Grid)      0.897898  0.849056  0.816901\n",
      "LR (L2) (Random)    0.895646  0.846632  0.816901\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'rocOutput'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[172], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[170], line 54\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(perfDF)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# save roc curves to data\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m rocDF\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrocOutput\u001b[49m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# store the best parameters\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mbestParamOutput, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'rocOutput'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS334",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
