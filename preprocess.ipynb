{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b2fcf89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d64fdabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(trainDF, testDF):\n",
    "    scaler = StandardScaler()\n",
    "    trainDF = scaler.fit_transform(trainDF)\n",
    "    testDF = scaler.fit_transform(testDF)\n",
    "    return trainDF, testDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7237e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_gridsearch(clf, pgrid, xTrain, yTrain, xTest, yTest):\n",
    "\n",
    "    cv = GridSearchCV(clf, param_grid=pgrid, cv=10)\n",
    "    cv.fit(xTrain, yTrain)\n",
    "\n",
    "    clf = cv.best_estimator_\n",
    "    best_params = cv.best_params_\n",
    "\n",
    "    yHat = cv.predict(xTest)\n",
    "    yHat_proba = cv.predict_proba(xTest)[:,1]\n",
    "\n",
    "    auc = roc_auc_score(yTest, yHat_proba)\n",
    "\n",
    "    auprc = average_precision_score(yTest, yHat_proba)\n",
    "\n",
    "    f1 = f1_score(yTest, yHat)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(yTest, yHat_proba)\n",
    "\n",
    "    return {'AUC': auc, 'AUPRC': auprc, 'F1': f1}, {'fpr': fpr, 'tpr': tpr}, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "103ff41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_randomsearch(clf, pgrid, xTrain, yTrain, xTest, yTest):\n",
    "    permutations = np.prod([len(v) for v in pgrid.values()])\n",
    "\n",
    "    cv = RandomizedSearchCV(clf, param_distributions=pgrid, n_iter=int(permutations*0.33), cv=10)\n",
    "    cv.fit(xTrain, yTrain)\n",
    "\n",
    "    clf = cv.best_estimator_\n",
    "    best_params = cv.best_params_\n",
    "\n",
    "    yHat = cv.predict(xTest)\n",
    "    yHat_proba = cv.predict_proba(xTest)[:,1]\n",
    "\n",
    "    auc = roc_auc_score(yTest, yHat_proba)\n",
    "\n",
    "    auprc = average_precision_score(yTest, yHat_proba)\n",
    "\n",
    "    f1 = f1_score(yTest, yHat)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(yTest, yHat_proba)\n",
    "\n",
    "    return {'AUC': auc, 'AUPRC': auprc, 'F1': f1}, {'fpr': fpr, 'tpr': tpr}, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "32972202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter_grid(mName):\n",
    "    if mName == 'LR (None)':\n",
    "        return {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'tol': [0.0001, 0.0004]}\n",
    "    elif mName == 'LR (L1)':\n",
    "        return {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'tol': [0.0001, 0.0004]}\n",
    "    elif mName == 'LR (L2)':\n",
    "        return {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'tol': [0.0001, 0.0004]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "896c807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_searchcv(clfName, clf, clfGrid,\n",
    "                  xTrain, yTrain, xTest, yTest,\n",
    "                  perfDict, rocDF, bestParamDict):\n",
    "    # evaluate grid search and add to perfDict\n",
    "    cls_perf, cls_roc, gs_p  = eval_gridsearch(clf, clfGrid, xTrain,\n",
    "                                               yTrain, xTest, yTest)\n",
    "    perfDict[clfName + \" (Grid)\"] = cls_perf\n",
    "    # add to ROC DF\n",
    "    rocRes = pd.DataFrame(cls_roc)\n",
    "    rocRes[\"model\"] = clfName\n",
    "    rocDF = pd.concat([rocDF, rocRes], ignore_index=True)\n",
    "    # evaluate random search and add to perfDict\n",
    "    clfr_perf, _, rs_p  = eval_randomsearch(clf, clfGrid, xTrain,\n",
    "                                            yTrain, xTest, yTest)\n",
    "    perfDict[clfName + \" (Random)\"] = clfr_perf\n",
    "    bestParamDict[clfName] = {\"Grid\": gs_p, \"Random\": rs_p}\n",
    "    return perfDict, rocDF, bestParamDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "145fe63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df = pd.read_csv(\"Models/Data/data.csv\")\n",
    "\n",
    "    # make classes\n",
    "    X = df.drop(columns=['close', 'otc'])\n",
    "    y = pd.DataFrame(columns=['price'])\n",
    "    for i, row in df.iterrows():\n",
    "        if row['open'] - row['close'] > 0:\n",
    "            y.loc[i] = 1 # decrease\n",
    "        else:\n",
    "            y.loc[i] = 0 # increase\n",
    "\n",
    "    y = y.to_numpy().flatten()\n",
    "\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    xTrain, xTest = preprocess(xTrain, xTest)\n",
    "\n",
    "    perfDict = {}\n",
    "    rocDF = pd.DataFrame()\n",
    "    bestParamDict = {}\n",
    "\n",
    "    print(\"Tuning Unregularized Logistic Regression --------\")\n",
    "    # logistic regression (unregularized)\n",
    "    unregLrName = \"LR (None)\"\n",
    "    unregLrGrid = get_parameter_grid(unregLrName)\n",
    "    # fill in\n",
    "    lrClf = LogisticRegression()\n",
    "    perfDict, rocDF, bestParamDict = eval_searchcv(unregLrName, lrClf, unregLrGrid,\n",
    "                                                   xTrain, yTrain, xTest, yTest,\n",
    "                                                   perfDict, rocDF, bestParamDict)\n",
    "    # logistic regression (L1)\n",
    "    print(\"Tuning Logistic Regression (Lasso) --------\")\n",
    "    lassoLrName = \"LR (L1)\"\n",
    "    lassoLrGrid = get_parameter_grid(lassoLrName)\n",
    "    # fill in\n",
    "    lassoClf = LogisticRegression(penalty='l1', solver='liblinear', max_iter=300)\n",
    "    perfDict, rocDF, bestParamDict = eval_searchcv(lassoLrName, lassoClf, lassoLrGrid,\n",
    "                                                   xTrain, yTrain, xTest, yTest,\n",
    "                                                   perfDict, rocDF, bestParamDict)\n",
    "    # Logistic regression (L2)\n",
    "    print(\"Tuning Logistic Regression (Ridge) --------\")\n",
    "    ridgeLrName = \"LR (L2)\"\n",
    "    ridgeLrGrid = get_parameter_grid(ridgeLrName)\n",
    "    # fill in\n",
    "    ridgeClf = LogisticRegression(penalty='l2')\n",
    "    perfDict, rocDF, bestParamDict = eval_searchcv(ridgeLrName, ridgeClf, ridgeLrGrid,\n",
    "                                                   xTrain, yTrain, xTest, yTest,\n",
    "                                                   perfDict, rocDF, bestParamDict)\n",
    "\n",
    "    perfDF = pd.DataFrame.from_dict(perfDict, orient='index')\n",
    "    print(perfDF)\n",
    "    # save roc curves to data\n",
    "    rocDF.to_csv('out', index=False)\n",
    "    # store the best parameters\n",
    "    with open('best', 'w') as f:\n",
    "        json.dump(bestParamDict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "069f3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout(model, xFeat, y, testSize):\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(xFeat, y, test_size=testSize)\n",
    "    resultDict = eval_randomsearch(model, xTrain, yTrain, xTest, yTest)\n",
    "    return resultDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "97b95a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Unregularized Logistic Regression --------\n",
      "Tuning Logistic Regression (Lasso) --------\n",
      "Tuning Logistic Regression (Ridge) --------\n",
      "                         AUC     AUPRC        F1\n",
      "LR (None) (Grid)    0.853030  0.772457  0.828571\n",
      "LR (None) (Random)  0.849242  0.765047  0.833333\n",
      "LR (L1) (Grid)      0.852273  0.768968  0.811594\n",
      "LR (L1) (Random)    0.852273  0.768968  0.811594\n",
      "LR (L2) (Grid)      0.853030  0.772457  0.828571\n",
      "LR (L2) (Random)    0.854545  0.778434  0.811594\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS334",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
